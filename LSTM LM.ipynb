{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Models\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "# dataset = datasets.load_dataset(\"monology/pile-uncopyrighted\")\n",
    "# Load a small subset of the dataset\n",
    "# dataset = datasets.load_dataset(\"monology/pile-uncopyrighted\", split=\"train[:1%]\")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "# train_test_split = dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "# test_valid_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "# dataset = {\"train\": train_test_split[\"train\"], \"validation\": test_valid_split[\"train\"], \"test\": test_valid_split[\"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb3e3488a734a2bbc29248fe9d4b3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb60955684544819f58ab7d62ed7c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6536123bb245d2bc2780c9589bfe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 13799838\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 13863\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/allenai/c4\n",
    "dataset = datasets.load_dataset(\"allenai/c4\", \"realnewslike\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 8831896\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 2207974\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 2759968\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "train_val_split = train_test_split[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "dataset[\"train\"] = train_val_split[\"train\"]\n",
    "dataset[\"validation\"] = train_val_split[\"test\"]\n",
    "dataset[\"test\"] = train_test_split[\"test\"]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the sample size of train, test, and validation sets\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(20000))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(2000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset[\"train\"].shape=(20000, 3)\n",
      "dataset['validation'].shape=(2000, 3)\n",
      "dataset['test'].shape=(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'{dataset[\"train\"].shape=}')\n",
    "print(f\"{dataset['validation'].shape=}\")\n",
    "print(f\"{dataset['test'].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "# tokenize_data = lambda example, tokenizer: {\"tokens\": tokenizer(example[\"text\"])}\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c516efc8a36b41a884ea016345e820d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5697d68189ea410b9961c1822436d0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c7985860ac4ce2b512c936b89e110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    def tokenize_data(example):\n",
    "        return {\"tokens\": tokenizer(example[\"text\"])}\n",
    "\n",
    "    tokenized_dataset = {split: data.map(tokenize_data, remove_columns=[\"text\"]) for split, data in dataset.items()}\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenize_dataset(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['timestamp', 'url', 'tokens'],\n",
      "    num_rows: 20000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['timestamp', 'url', 'tokens'],\n",
      "    num_rows: 2000\n",
      "}), 'test': Dataset({\n",
      "    features: ['timestamp', 'url', 'tokens'],\n",
      "    num_rows: 2000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['several', 'signals', 'of', 'progress', 'in', 'washington', 'pushed', 'stocks', 'higher', 'on', 'monday', '.', 'there', 'are', 'just', 'three', 'days', 'before', 'the', 'us', 'hits', 'the', 'debt', 'ceiling', 'and', 'investors', 'are', 'betting', 'that', 'a', 'deal', 'will', 'soon', 'be', 'reached', '.', 'netflix', '(', 'nflx', ')', 'was', 'trending', 'today', 'on', 'word', 'that', 'the', 'company', 'may', 'be', 'negotiating', 'a', 'cable', 'deal', 'with', 'major', 'operators', 'in', 'the', 'us', '.', 'macy', \"'\", 's', '(', 'm', ')', 'announced', 'that', 'they', 'will', 'be', 'opening', 'on', 'thanksgiving', 'for', 'the', 'first', 'time', '.', 'expedia', '(', 'expe', ')', 'fell', 'almost', '7%', 'on', 'a', 'recent', 'stock', 'downgrade', '.', 'many', 'are', 'looking', 'ahead', 'to', 'quarterly', 'reports', 'from', 'citigroup', '(', 'c', ')', ',', 'coca-cola', '(', 'ko', ')', 'and', 'intel', '(', 'intc', ')', ',', 'all', 'scheduled', 'for', 'tuesday', '.', 'the', 'chinese', 'yuan', 'hit', 'a', 'new', 'record', 'high', 'against', 'the', 'dollar', 'while', 'the', 'us', 'currency', 'reached', 'an', 'intraday', 'high', 'against', 'the', 'japanese', 'yen', '.', 'progress', 'possible', 'in', 'senate', ',', 'leaders', 'optimistic', 'that', 'a', 'debt', 'deal', 'may', 'be', 'reached', '.', 'shares', 'of', 'netflix', '(', 'nflx', ')', 'jump', '5%', 'on', 'talks', 'of', 'us', 'cable', 'options', '.', 'chinese', 'inflation', 'reaches', 'a', 'seven-month', 'high', 'on', 'higher', 'food', 'prices', 'and', 'drop', 'in', 'exports', '.', 'apple', '(', 'aapl', ')', 'notes', 'that', 'iphone', '5c', 'inventory', 'is', 'moving', ',', 'and', '5s', 'supply', 'is', 'tight', '.', 'eurozone', 'industrial', 'output', 'rebounds', 'and', 'spurs', 'hope', 'for', 'higher', 'economic', 'growth', '.', 'gilead', 'sciences', '(', 'gild', ')', 'rallies', 'after', 'ending', 'a', 'late-stage', 'trial', 'ahead', 'of', 'schedule', '.', 'the', 'government', 'shutdown', '-', 'is', 'there', 'a', 'simple', 'solution', 'that', \"'\", 's', 'being', 'overlooked', '?', 'why', 'this', 'finance', 'course', 'should', 'be', '(', 'and', 'now', 'is', ')', 'available', 'to', 'the', 'masses', '.', 'budget', 'deal', 'hopes', 'push', 'markets', 'higher', 'on', 'monday', '(', 'nflx', ',', 'aapl', ')', '.', 'investorguide', '.', 'com', '.', 'webfinance', ',', 'inc', '.', 'http', '//www', '.', 'investorguide', '.', 'com/article/14086/budget-deal-hopes-push-markets-higher-on-monday-nflx-aapl-igd/', '(', 'access', 'april', '24', ',', '2019', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][100][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset[\"train\"][\"tokens\"], min_freq=3)\n",
    "vocab.insert_token(\"<unk>\", 0)\n",
    "vocab.insert_token(\"<eos>\", 1)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74619\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', 'the', '.', ',', 'to', 'and', 'of', 'a', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example[\"tokens\"]:\n",
    "            tokens = example[\"tokens\"].append(\"<eos>\")\n",
    "            tokens = [vocab[token] for token in example[\"tokens\"]]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[: num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches)  # view vs. reshape (whether data is contiguous)\n",
    "    return data  # [batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = get_data(tokenized_dataset[\"train\"], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset[\"validation\"], vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset[\"test\"], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape=torch.Size([64, 154421])\n",
      "valid_data.shape=torch.Size([64, 15844])\n",
      "test_data.shape=torch.Size([64, 15723])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_data.shape=}\")\n",
    "print(f\"{valid_data.shape=}\")\n",
    "print(f\"{test_data.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/LM.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1 / math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)  # We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)  # Wh\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()  # not to be used for gradient computation\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        # src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))  # harry potter is\n",
    "        # embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        # ouput: [batch size, seq len, hid dim]\n",
    "        # hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        # prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024  # 400 in the paper\n",
    "hid_dim = 1024  # 1150 in the paper\n",
    "num_layers = 2  # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 169,687,931 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    src = data[:, idx : idx + seq_len]\n",
    "    target = data[:, idx + 1 : idx + seq_len + 1]  # target simply is ahead of src by 1\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, : num_batches - (num_batches - 1) % seq_len]  # we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    # reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc=\"Training: \", leave=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx)  # src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)\n",
    "\n",
    "        # need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  # prediction: [batch size * seq len, vocab size]\n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, : num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size = src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3088/3088 [26:11<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 630.561\n",
      "\tValid Perplexity: 344.386\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "seq_len = 50  # <----decoding length\n",
    "clip = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # torch.save(model.state_dict(), \"best-val-lstm_lm.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"emb_dim\": emb_dim,\n",
    "                \"hid_dim\": hid_dim,\n",
    "                \"num_layers\": num_layers,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "            },\n",
    "            \"best-val-lstm_lm.pt\",\n",
    "        )\n",
    "\n",
    "    print(f\"\\tTrain Perplexity: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"\\tValid Perplexity: {math.exp(valid_loss):.3f}\")\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"vocab\": vocab,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"hid_dim\": hid_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "        },\n",
    "        \"best-val-lstm_lm.pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', 'the', '.', ',', 'to', 'and', 'of', 'a', 'in']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"best-val-lstm_lm.pt\", weights_only=False, map_location=device)\n",
    "model = LSTMLanguageModel(\n",
    "    checkpoint[\"vocab_size\"], checkpoint[\"emb_dim\"], checkpoint[\"hid_dim\"], checkpoint[\"num_layers\"], checkpoint[\"dropout_rate\"]\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "vocab = checkpoint[\"vocab\"]\n",
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 339.976\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f\"Test Perplexity: {math.exp(test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            # prediction: [batch size, seq len, vocab size]\n",
    "            # prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            while prediction == vocab[\"<unk>\"]:  # if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab[\"<eos>\"]:  # if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)  # autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "donald trump has been able to be a lot of people who have been able to be able to be a lot of people who have been able to be able to be\n",
      "\n",
      "0.2\n",
      "donald trump has been able to be a little of the city . the company said the company was a good , and the same , and the company ' s brother ,\n",
      "\n",
      "0.3\n",
      "donald trump has been able to be a little of the city . in the statement , the department of the state ' s most important thing , the end of the jury\n",
      "\n",
      "0.4\n",
      "donald trump has been able to be a little of the city . in the statement , the department of the bower was the first and had been able to have a nod\n",
      "\n",
      "0.5\n",
      "donald trump has been an investment for the time of the world . the architect of the united states would be the written one of the press of the life-threatening funds . now\n",
      "\n",
      "0.6\n",
      "donald trump has been not a nicky challenging time for them . i ' ve heard . i ' ve been the written one of the press song , she said . now\n",
      "\n",
      "0.7\n",
      "donald trump has been not reflected by the adaptive headwind of part . all to have been included by these and the bower f . jacob and song flipped the end . now\n",
      "\n",
      "0.8\n",
      "donald trump has been not reflected by the adaptive headwind of part . all to have been included by these and births written by liquidation county and song premiers life-threatening with ties with\n",
      "\n",
      "0.9\n",
      "donald trump has been canceled by nicky segmentation . the battery congregation . all refers to art included adam carl and minn bower f . jacob 2-3 ' s life-threatening explores ties with\n",
      "\n",
      "1.0\n",
      "donald trump has been canceled phone nicky segmentation quietly headwind williams congregation . all refers to art included adam “ms and minn bower f . partick 2-3 song premiers life-threatening harvests ties with\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Donald Trump has\"\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# smaller the temperature, more diverse tokens but comes\n",
    "# with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "    print(str(temperature) + \"\\n\" + \" \".join(generation) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
